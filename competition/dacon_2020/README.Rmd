---
title: "2020 Dacon"
subtitle: "원자력 발전소 상태 예측"
output: rmarkdown::github_document
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(fig.path = "README_figs/README-")
```

## 1. 프로젝트 목표

- 안전한 원자력 발전을 위해 한국수력원자원에서 제공한 모의 운전 및 실제 데이터를 기반으로, **원자력 발전소 상태를 예측**하는 알고리즘 개발

- multi-category log loss를 이용하여 모델 성능을 평가


## 2. 프로젝트 특징

### 1)  독특한 데이터 구조
- Train set는 약 800여개의 Sub-dataset으로 구성되어 있으며, 각 'Dataset에 class가 할당'되어 있음
- 개별 Dataset들은 60초 동안 초 단위로 기록된 5000여개의 변수로 구성되어 있음 (60x5000)
- 이 때 0~15초 사이에 상태 변화가 일어나며, 바로 이 상태가 예측하고자 하는 타겟 변수임

### 2) 클래스가 매우 많고 불균형한 분류 문제
- 800여개의 Dataset이 197개의 클래스를 가지고 있음
- 이 중 유일한 한 개의 클래스로 할당된 Dataset이 80여개에 달함


## 3. 프로젝트 과정

### 1) 상태 변화시점 예측
- 상태를 예측하기에 앞서, 0~15초 사이에 일어난다고 공지된 상태변화 시점을 예측하는 것이 중요하다고 판단
- 만약 상태 변화가 일어났다면 해당 시점과 그 이전 시점의 데이터 사이에 큰 변화가 있을 것이라고 가정
- 행 간 데이터 사이의 거리를 나타내는 변수를 만들고 시각화 하여 패턴을 관찰
- 시각화 결과, 이 변수만으로 상태변화 시점이 명확히 드러나진 않으나, 유사한 패턴을 보이는 Cluster들이 있음을 발견
- K-means 알고리즘을 사용하자 실제로 잘 Clustering이 되었고, 이 Cluster들을 모델링 변수로 사용

### 2) 모델링
- 무엇보다 변수(5000여개)를 줄이는 것이 중요하다고 판단하여 차원 축소에 신경을 많이 씀
- 결측치 및 상관성이 1인 변수 등 불필요한 변수들 제거
- Random forest와 Extra tree로 모델링하여 Feauture Importance 상위 200개의 중복되는 변수들을 사용
- Random forest / Extra tree / LGB 세 가지 모델의 예측 확률을 가중평균한 Soft voting classifier 모델 사용
- Random forest와 Extra tree를 사용할 때에 비해 LGB 모델을 추가하자 예측 성능이  꽤 향상되었음
- 이는 클래스가 많고 불균형하기 때문에 오히려 과적합하는 모델(LGB)이 좋은 성능을 보이는 것이라고 예상

### 3) 예측 확률값 보정
- Validation Set을 이용해 예측 결과들의 분포를 조사한 결과, 특히 잘 맞추거나 잘 맞추지 못하는 클래스 군이 있음을 발견
- log loss 스코어를 높여주기 위해, 잘 맞추는 클래스로 예측한 확률들을 상향, 반대의 경우는 하향하여 확률값을 보정


## 4. 프로젝트 결과

- 대쉬보드 기준 188팀 중 9등


## 5. 나의 역할

- 상태 변화시점 예측 전담
- LGB 모델 하이퍼파라미터 튜닝
- 예측 확률값 보정

